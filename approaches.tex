\chapter{Approaches} \label{chap:approaches}

This chapter will describe the main dataset used for this research, why it was chosen,
the main EMG signals of interest from the dataset, methods of feature selection and
visualisation and the machine learning approaches used to transcribe the EMG data
into a text.

\section{Dataset}

\subsection{Dataset Selection and Justification}

The dataset which is used throughout this research project is the open-source
surface electromyography silent speech (sEMG silent speech) dataset released
by David Gaddy along with his paper, Digital Voicing of Silent Speech
(\cite{gaddy2020digital}).
The paper describes a novel method of transcribing aligned silent speech data
directly into speech features along with the largest open-source sEMG silent
speech dataset.

This dataset was chosen for this research as it is the largest, quality open
source sEMG silent speech dataset.

\subsection{Dataset Slices}

For the experiments, the original dataset was split into two main groups:
the closed vocabulary slice and the full dataset. The reason for this was
that performing experiments on the entire dataset was likely to be very
time consuming so performing experiments on a subset of the entire dataset
which had a much smaller distribution of words heavily decreased training time
(as the closed vocab condition only comprises roughly 5\% of the dataset) but
the dataset still contains a relatively equal distribution of characters within
the dataset.

\subsection{Feature Selection}

For this research there were two primary ways of selecting features. The first method
was to use the same feature processing methods described in the original
(\cite{gaddy2020digital}) paper. The second method was to use a convolutional
neural network (CNN) architecture to automatically learn features from the
dataset, in an end-to-end manner.

\section{Models}

\subsection{DeepSpeech2 Model}

The DeepSpeech2 model was the initial automatic speech recognition (ASR) machine
learning model which was considered for experimentation as it is relatively simple to
implement and is known to have good performance, even on smaller datasets, achieving
a 3.10 WER on the WSJ eval'92 dataset
(\cite{DS2_original}).

\section{Baseline ASR Testing}

To get a realistic baseline for the possible performance of the silent
speech models, standard audio based ASR models were trained on different
slices of the Digital Voicing datasets audio and text transcriptions only.
The intuition for this was that an ASR model trained on the audio and text
transcriptions should in theory perform better than on the purely vocalised
EMG data or final silent EMG data and their respective text transcriptions.
The results of the baseline ASR models is provided below.

\subsection{Implementation}

\subsection{Results}

The following results table shows the word-error rate for each dataset
which the DeepSpeech2 model was trained on. Each dataset used a slightly
different vocabulary. The vocabulary for each is the list of valid characters
of the text transcriptions which the model considers, any other characters
are encoded as unknown.

\begin{center}
\captionof{table}{DeepSpeech2 Audio ASR Baseline Results}
\begin{tabular} { | l | c | c | }
Dataset Slice & Vocabulary & WER \\
\hline
Closed Vocab & " abcdefghijklmnopqrstuvwxyz0123456789-" & 0.33 \\
Full Vocalised Dataset & " abcdefghijklmnopqrstuvwxyz-" & 0.45
\end{tabular}
\end{center}