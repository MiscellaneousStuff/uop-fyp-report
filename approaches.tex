\chapter{Approaches} \label{chap:approaches}

This chapter will describe the main dataset used for this research, why it was chosen,
the main EMG signals of interest from the dataset, methods of feature selection and
visualisation and the machine learning approaches used to transcribe the EMG data
into a text transcription.

\section{Fine Tuning Speech Recognition on Model Predictions}

\subsection{Closed Vocabulary Dataset}

The intuition behind fine tuning a speech recognition model on the
predicted mel spectrograms predicted by the transduction model comes
from the literature for speech recogntion. Typical speech recognition models
suffer from a loss of accuracy when they are only trained on clean audio data
and then evaluated on a noisy dataset.

This issue is also reflected in the transduction task. The state-of-the-art
transduction model within the Improved Voicing (\cite{gaddy2021improved}) paper
will always produce audio outputs with artefacts because of the final vocoder layer.

% (Desc.) UoP: FYP: SEMG ASR (Finetune DS2 on Transducer Outputs #8)
{\small\begin{center}
\captionof{table}{DeepSpeech2 Silent Speech ASR Results}
\begin{tabular} {  l  c  c  }
\hline
\textbf{Dataset} & \textbf{CER} & \textbf{WER} \\
\hline
Ground Truth (GT) & 87.10 & 100.50 \\
Voiced & 51.27 & 87.33 \\
Silent & 38.80 & 78.10 \\
Silent, Voiced & \textbf{35.26} & 75.33 \\
\hline
Silent, Voiced, GT & 35.72 & \textbf{70.83} \\
\hline
\end{tabular}
\end{center}}

\subsection{Open Vocabulary (Parallel-Only) Dataset}



\iffalse

\section{Dataset}

\subsection{Dataset Selection and Justification}

The dataset which is used throughout this research project is the open-source
surface electromyography silent speech (sEMG silent speech) dataset released
by David Gaddy along with his paper, Digital Voicing of Silent Speech
(\cite{gaddy2020digital}).
The paper describes a novel method of transcribing aligned silent speech data
directly into speech features along with the largest open-source sEMG silent
speech dataset.

This dataset was chosen for this research as it is the largest, high quality open
source sEMG silent speech dataset.

\subsection{Feature Selection}

For this research there were two primary ways of selecting features. The first method
was to use the same feature processing methods described in the original
(\cite{gaddy2020digital}) paper. The second method was to use a convolutional
neural network (CNN) architecture to automatically learn features from the
dataset, in an end-to-end manner.

\section{Models}

\subsection{DeepSpeech2 Model}

The DeepSpeech2 model was the initial automatic speech recognition (ASR) machine
learning model which was considered for experimentation as it is relatively simple to
implement and is known to have good performance, even on smaller datasets, achieving
a 3.10 WER on the WSJ eval'92 dataset
(\cite{DS2_original}).

\section{Baseline Audio ASR Testing}

To get a realistic baseline for the possible performance of the silent
speech models, standard audio based ASR models were trained on different
slices of the Digital Voicing datasets audio and text transcriptions only.
The intuition for this was that an ASR model trained on the audio and text
transcriptions should in theory perform better than on the purely vocalised
EMG data or final silent EMG data and their respective text transcriptions.
The results of the baseline ASR models is provided below.

\subsection{Datasets}

Three main datasets were chosen for the baseline ASR tests. The first dataset
was the LJSpeech dataset
(\cite{ljspeech17})
which is an open-source dataset primarily used in
speech synthesis and also speech recognition. The remaining two datasets
are comprised of the audio and text transcriptions of vocalised
utterances from the Digital Voicing dataset release (\cite{gaddy2020digital}).
The first dataset from this data comes from utterances from the closed
vocabulary set of recordings during the vocalised condition from the dataset.
The second dataset from this data comes from
all of the audio and text transcriptions during the vocalised condition across
the entire dataset. The three datasets are referred to as
\textit{ASR-LJSpeech}, \textit{ASR-SilentSpeechVocal-Closed} and
\textit{ASR-SilentSpeechVocal-Full},
respectively.

The \textit{ASR-LJSpeech dataset} was chosen as it is a standard open-source dataset
used for speech related machine learning tasks. It also has similar, but greater
quantity of important properties than the \textit{ASR-SilentSpeechVocal-Full} dataset.
These include the vocabulary size,
average length of recordings and duration of the entire dataset. This intuitively
means that the DeepSpeech2 ASR model trained on the \textit{ASR-LJSpeech dataset} should
have a lower WER rate, so better performance, than trained on
the \textit{ASR-SilentSpeechVocal-Full dataset}.
Whereas the \textit{ASR-SilentSpeechVocal-Closed} dataset was chosen because it has
a far lower duration and vocabulary size than the \textit{ASR-SilentSpeechVocal-Full}
dataset which makes it a lot faster to experiment with as the training time is
lower.

\subsection{Implementation}

The final DeepSpeech2 audio ASR model and training code is adapted from
online simplified examples of the original DeepSpeech2 implementation by Baidu.
Modern improvements to the model and pre-processing of the data are implemented.
These include transforming the audio waveforms into mel spectrograms and
occasionally masking parts of the spectrogram along frequency and time.

This
helps regularise the model by periodically zeroing out parts of the input.
This is similar to another regularisation technique often used in training
neural networks, known as Dropout. The goal is to reduce overfitting by preventing
complex co-adaptations on training data
(\cite{pmlr-v28-wan13}).

Another method which is used to greatly increase training speed and increase
batch size is Automatic Mixed Precision. Deep neural network training has
normally relied on IEEE single-precision format (fp32), however mixed
precision allows training with half precision (fp16, or bfloat16) while
maintaining the accuracy of single precision.

\subsection{Results}

The following results table shows the word-error rate for each dataset
which the DeepSpeech2 model was trained on. Each dataset used a slightly
different vocabulary. The vocabulary for each dataset
is the list of valid characters
of the text transcriptions which the model considers, any other characters
are encoded as \textless UNK\textgreater. A different vocabulary was chosen for the
ASR-SilentSpeechVocal-Closed dataset because it contains far more numbers
than the other datasets and including numbers in it's vocabulary means
that the transcriptions would be useful, rather than filled with the
unknown placeholder value.

% ALL OF THESE EXPERIMENTS NEED TO BE RE-RUN AT SOME POINT!

{\small\begin{center}
\captionof{table}{DeepSpeech2 Audio ASR Baseline Results}
\begin{tabular} { | l | c | c | }
\hline
Dataset & Encoding Vocabulary & WER \\
\hline
ASR-LJSpeech                 & " abcdefghijklmnopqrstuvwxyz-" & 0.55 \\
ASR-SilentSpeechVocal-Closed & " abcdefghijklmnopqrstuvwxyz0123456789-" & 0.33 \\
ASR-SilentSpeechVocal-Full   & " abcdefghijklmnopqrstuvwxyz-" & 0.45 \\
\hline
\end{tabular}
\end{center}}

\section{Vocalised EMG ASR Testing}

After training the DeepSpeech2 baseline audio ASR models, experiments
began on the vocalised EMG condition. This includes all of the utterances
in the Digital Voicing dataset where the speaker strongly articulated
what they were trying to say, or in other words, vocalised speech.
This means that the EMG signal is much stronger compared to the silent
speech condition. In theory, a model trained and evaluated on the vocalised
speech condition should show stronger performance than the silent speech
condition, due to the richer underlying signal. This means that the a dataset
comprised of purely vocalised utterances should provide a strong maximum
performance which the purely silent utterances are unlikely to be better
than.

\subsection{Implementation}

The first attempt at porting the DeepSpeech2 model from the audio ASR
condition to the vocalised EMG ASR condition involved removing the initial
CNN layers from the DeepSpeech2 model, as it was easier to just provide
the hand-crafted features from the Digital Voicing paper at first
(\cite{gaddy2020digital}). This is because it is harder to diagnose an
end-to-end deep learning model as it's not easy to visualise what the
model is doing. Then a dataset of just one sample of vocalised EMG
data from the closed vocabulary condition was setup as a training
and testing dataset. This was done to see whether or not the model
could overfit to the data. This is a standard practice done, especially
when experimenting with novel neural network architectures or domains,
because if a model cannot overfit to a single example in a dataset,
it has no chance of generalising across multiple examples and producing
useful behaviour. This is referred to as the sanity test from here on.

\fi