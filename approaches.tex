\chapter{Approaches} \label{chap:approaches}

This chapter will describe the main dataset used for this research, why it was chosen,
the main EMG signals of interest from the dataset, methods of feature selection and
visualisation and the machine learning approaches used to transcribe the EMG data
into a text transcription.

\section{Dataset}

\subsection{Dataset Selection and Justification}

The dataset which is used throughout this research project is the open-source
surface electromyography silent speech (sEMG silent speech) dataset released
by David Gaddy along with his paper, Digital Voicing of Silent Speech
(\cite{gaddy2020digital}).
The paper describes a novel method of transcribing aligned silent speech data
directly into speech features along with the largest open-source sEMG silent
speech dataset.

This dataset was chosen for this research as it is the largest, high quality open
source sEMG silent speech dataset.

\subsection{Feature Selection}

For this research there were two primary ways of selecting features. The first method
was to use the same feature processing methods described in the original
(\cite{gaddy2020digital}) paper. The second method was to use a convolutional
neural network (CNN) architecture to automatically learn features from the
dataset, in an end-to-end manner.

\section{Models}

\subsection{DeepSpeech2 Model}

The DeepSpeech2 model was the initial automatic speech recognition (ASR) machine
learning model which was considered for experimentation as it is relatively simple to
implement and is known to have good performance, even on smaller datasets, achieving
a 3.10 WER on the WSJ eval'92 dataset
(\cite{DS2_original}).

\section{Baseline ASR Testing}

To get a realistic baseline for the possible performance of the silent
speech models, standard audio based ASR models were trained on different
slices of the Digital Voicing datasets audio and text transcriptions only.
The intuition for this was that an ASR model trained on the audio and text
transcriptions should in theory perform better than on the purely vocalised
EMG data or final silent EMG data and their respective text transcriptions.
The results of the baseline ASR models is provided below.

\subsection{Datasets}

Three main datasets were chosen for the baseline ASR tests. The first dataset
was the LJSpeech dataset
(\cite{ljspeech17})
which is an open-source dataset primarily used in
speech synthesis and also speech recognition. The remaining two datasets
are comprised of the audio and text transcriptions of vocalised
utterances from the Digital Voicing dataset release (\cite{gaddy2020digital}).
The first dataset from this data comes from utterances from the closed
vocabulary set of recordings during the vocalised condition from the dataset.
The second dataset from this data comes from
all of the audio and text transcriptions during the vocalised condition across
the entire dataset. The three datasets are referred to as
\textit{ASR-LJSpeech}, \textit{ASR-SilentSpeechVocal-Closed} and
\textit{ASR-SilentSpeechVocal-Full},
respectively.

The \textit{ASR-LJSpeech dataset} was chosen as it is a standard open-source dataset
used for speech related machine learning tasks. It also has similar, but greater
quantity of important properties than the \textit{ASR-SilentSpeechVocal-Full} dataset.
These include the vocabulary size,
average length of recordings and duration of the entire dataset. This intuitively
means that the DeepSpeech2 ASR model trained on the \textit{ASR-LJSpeech dataset} should
have a lower WER rate, so better performance, than trained on
the \textit{ASR-SilentSpeechVocal-Full dataset}.
Whereas the \textit{ASR-SilentSpeechVocal-Closed} dataset was chosen because it has
a far lower duration and vocabulary size than the \textit{ASR-SilentSpeechVocal-Full}
dataset which makes it a lot faster to experiment with as the training time is
lower.

\subsection{Implementation}

\subsection{Results}

The following results table shows the word-error rate for each dataset
which the DeepSpeech2 model was trained on. Each dataset used a slightly
different vocabulary. The vocabulary for each dataset
is the list of valid characters
of the text transcriptions which the model considers, any other characters
are encoded as \textless UNK\textgreater. A different vocabulary was chosen for the
ASR-SilentSpeechVocal-Closed dataset because it contains far more numbers
than the other datasets and including numbers in it's vocabulary means
that the transcriptions would be useful, rather than filled with the
unknown placeholder value.

% ALL OF THESE EXPERIMENTS NEED TO BE RE-RUN AT SOME POINT!

{\small\begin{center}
\captionof{table}{DeepSpeech2 Audio ASR Baseline Results}
\begin{tabular} { | l | c | c | }
\hline
Dataset & Encoding Vocabulary & WER \\
\hline
ASR-LJSpeech                 & " abcdefghijklmnopqrstuvwxyz-" & 0.55 \\
ASR-SilentSpeechVocal-Closed & " abcdefghijklmnopqrstuvwxyz0123456789-" & 0.33 \\
ASR-SilentSpeechVocal-Full   & " abcdefghijklmnopqrstuvwxyz-" & 0.45 \\
\hline
\end{tabular}
\end{center}}