Dataset - LJSpeech
@misc{ljspeech17,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}

Literature Review - Conformer Architecture
@misc{conformer,
  doi = {10.48550/ARXIV.2005.08100},
  url = {https://arxiv.org/abs/2005.08100},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  publisher = {arXiv},
  year = {2020},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


Literature Review - DeepSpeech2 Architecture
@misc{DS2_original,
  doi = {10.48550/ARXIV.1512.02595},
  url = {https://arxiv.org/abs/1512.02595},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Speech 2: End-to-End Speech Recognition in English and Mandarin},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


Literature Review - Original sEMG Systems
@article{lit_review_semg_history,
      author = {Wang, You and Zhang, Ming and Wu, Rumeng and Gao, Han and Yang, Meng and Luo, Zhiyuan and Li, Guang},
      year = {2020},
      month = {07},
      pages = {442},
      title = {Silent Speech Decoding Using Spectrogram Features Based on Neuromuscular Activities},
      volume = {10},
      journal = {Brain Sciences},
      doi = {10.3390/brainsci10070442}
}

Literature Review - Pruning
@misc{liebenwein2021lost,
      title={Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}, 
      author={Lucas Liebenwein and Cenk Baykal and Brandon Carter and David Gifford and Daniela Rus},
      year={2021},
      eprint={2103.03014},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

Literature Review - Quantization
@misc{wu2020integer,
      title={Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation}, 
      author={Hao Wu and Patrick Judd and Xiaojie Zhang and Mikhail Isaev and Paulius Micikevicius},
      year={2020},
      eprint={2004.09602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

OpenAI 2018 BlogPost - Details useful tools and other information which is used for the MLOps side of the project
@misc{OpenAI_dota,
  author = {OpenAI},
  title = {OpenAI Five},
  howpublished = {\url{https://web.archive.org/web/20210422020001/https://openai.com/blog/openai-five/}},
  year = {2018}
}

Efficient CapsNet - Might be very useful in the feature detection aspect of EEG dimensionality reduction and correlating features before
passing them on to a recurrent / transformer network
@misc{mazzia2021efficientcapsnet,
      title={Efficient-CapsNet: Capsule Network with Self-Attention Routing}, 
      author={Vittorio Mazzia and Francesco Salvetti and Marcello Chiaberge},
      year={2021},
      eprint={2101.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

Original CapsNet paper by Hinton, et al.
@misc{sabour2017dynamic,
      title={Dynamic Routing Between Capsules}, 
      author={Sara Sabour and Nicholas Frosst and Geoffrey E Hinton},
      year={2017},
      eprint={1710.09829},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

EfficientNet - Automatically adjusts baseline image classification network based on
required attributes of new model
@misc{tan2020efficientnet,
      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2020},
      eprint={1905.11946},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

ResNet - Original Paper, refer to this when describing digital voicing papers
and our new model which will definitely use some variation of this
@INPROCEEDINGS{resnet, 
 author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  
 booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
 title={Deep Residual Learning for Image Recognition},   
 year={2016},  
 volume={},  
 number={},  
 pages={770-778},  
 doi={10.1109/CVPR.2016.90}
}

Dota 2 Open AI Five Paper - Contains lots of generally useful information about large scale ML ideas
- Compute Usage formula taken from here
@misc{openai2019dota,
      title={Dota 2 with Large Scale Deep Reinforcement Learning}, 
      author={OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
      year={2019},
      eprint={1912.06680},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

ZFNet - Improves upon AlexNet and first visualisation of CNNs for image classification models
Also provides info for diagnosing performance of CNN image classification models
@misc{zeiler2013visualizing,
      title={Visualizing and Understanding Convolutional Networks}, 
      author={Matthew D Zeiler and Rob Fergus},
      year={2013},
      eprint={1311.2901},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

Levenshtein Distance (edit-distance between two words)
@ARTICLE{1966SPhD...10..707L,
       author = {{Levenshtein}, V.~I.},
        title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
      journal = {Soviet Physics Doklady},
         year = 1966,
        month = feb,
       volume = {10},
        pages = {707},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

SSI for Speech Restoration (Meta-Review of main SSIs, approaches, to-speech and to-text, biosignals)
Use this as a central resource to find references for specific SSI issues and to get a good overview of the silent speech literature in general
@article{ssi_meta_review,
  author = {Pérez-Córdoba, José and Martín-Doñas, Juan and Gomez, Angel and Gomez-Alanis, Alejandro and Gonzalez Lopez, Jose},
  year = {2020},
  month = {09},
  pages = {177995-178021},
  title = {Silent Speech Interfaces for Speech Restoration: A Review},
  volume = {8},
  journal = {IEEE Access},
  doi = {10.1109/ACCESS.2020.3026579}
}

Another potentially useful meta-review of biosignal sensors and DL-based speech recognition
@Article{s21041399,
  AUTHOR = {Lee, Wookey and Seong, Jessica Jiwon and Ozlu, Busra and Shim, Bong Sup and Marakhimov, Azizbek and Lee, Suan},
  TITLE = {Biosignal Sensors and Deep Learning-Based Speech Recognition: A Review},
  JOURNAL = {Sensors},
  VOLUME = {21},
  YEAR = {2021},
  NUMBER = {4},
  ARTICLE-NUMBER = {1399},
  URL = {https://www.mdpi.com/1424-8220/21/4/1399},
  ISSN = {1424-8220},
  ABSTRACT = {Voice is one of the essential mechanisms for communicating and expressing one’s intentions as a human being. There are several causes of voice inability, including disease, accident, vocal abuse, medical surgery, ageing, and environmental pollution, and the risk of voice loss continues to increase. Novel approaches should have been developed for speech recognition and production because that would seriously undermine the quality of life and sometimes leads to isolation from society. In this review, we survey mouth interface technologies which are mouth-mounted devices for speech recognition, production, and volitional control, and the corresponding research to develop artificial mouth technologies based on various sensors, including electromyography (EMG), electroencephalography (EEG), electropalatography (EPG), electromagnetic articulography (EMA), permanent magnet articulography (PMA), gyros, images and 3-axial magnetic sensors, especially with deep learning techniques. We especially research various deep learning technologies related to voice recognition, including visual speech recognition, silent speech interface, and analyze its flow, and systematize them into a taxonomy. Finally, we discuss methods to solve the communication problems of people with disabilities in speaking and future research with respect to deep learning components.},
  DOI = {10.3390/s21041399}
}

Digital Voicing of Silent Speech
@misc{gaddy2020digital,
      title={Digital Voicing of Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2020},
      eprint={2010.02960},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

Improves upon the original paper by:
- Replacing LSTMs with transformer network
- Generating time-aligned phoneme info from audio and using it as training signal
- Replace hand-designed features with convolution features
@misc{gaddy2021improved,
      title={An Improved Model for Voicing Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2021},
      eprint={2106.01933},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

Main MIT AlterEgo 2018 Project
@inproceedings{10.1145/3172944.3172977,
  author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
  title = {AlterEgo: A Personalized Wearable Silent Speech Interface},
  year = {2018},
  isbn = {9781450349451},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3172944.3172977},
  doi = {10.1145/3172944.3172977},
  abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92% median word accuracy levels.},
  booktitle = {23rd International Conference on Intelligent User Interfaces},
  pages = {43–53},
  numpages = {11},
  keywords = {intelligence augmentation, silent speech interface, human-machine symbiosis, peripheral nerve interface},
  location = {Tokyo, Japan},
  series = {IUI '18}
}

60 Page In-Depth Discussion of MIT AlterEgo 2018 Project
@inproceedings{Kapur2018HumanmachineCC,
  title={Human-machine cognitive coalescence through an internal duplex interface},
  author={Arnav Kapur},
  year={2018}
}

MIT AlterEgo Dysphonia Research (Silent speech in ambulatory setting with Multiple Sclerosis patients)
@InProceedings{pmlr-v116-kapur20a,
  title = {{Non-Invasive Silent Speech Recognition in Multiple Sclerosis with Dysphonia}},
  author = {Kapur, Arnav and Sarawgi, Utkarsh and Wadkins, Eric and Wu, Matthew and Hollenstein, Nora and Maes, Pattie},
  booktitle = {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = {25--38},
  year = {2020},
  editor = {Adrian V. Dalca and Matthew B.A. McDermott and Emily Alsentzer and Samuel G. Finlayson and Michael Oberst and Fabian Falck and Brett Beaulieu-Jones},
  volume = {116},
  series = {Proceedings of Machine Learning Research},
  address = {}, 
  month = {13 Dec}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v116/kapur20a/kapur20a.pdf}, 
  url = {http://proceedings.mlr.press/v116/kapur20a.html}, 
  abstract = {We present the first non-invasive real-time silent speech system that helps patients with speech disorders to communicate in natural language voicelessly, merely by articulating words or sentences in the mouth without producing any sounds. We collected neuromus-cular recordings to build a dataset of 10 trials of 15 sentences from each of 3 multiple sclerosis (MS) patients with dysphonia, spanning a range of severity and subsequently affected speech attributes. We present a pipeline wherein we carefully preprocess the data, develop a convolutional neural architecture and employ personalized machine learning. In our experiments with multiple sclerosis patients, our system achieves a mean overall test accuracy of 0.81 at a mean information transfer rate of 203.73 bits per minute averaged over all patients. Our work demonstrates the potential of a reliable and promising human-computer interface that classifies intended sentences from silent speech and hence, paves the path for future work with further speech disorders in conditions such as amyotrophic lateral sclerosis (ALS), stroke, and oral cancer, among others.}
}