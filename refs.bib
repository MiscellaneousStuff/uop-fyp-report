speech delay paper 1
@article{doi:10.2466/23.25.PMS.120v17x2,
author = {Andrew Stuart and Joseph Kalinowski},
title ={Effect of Delayed Auditory Feedback, Speech Rate, and Sex on Speech Production},
journal = {Perceptual and Motor Skills},
volume = {120},
number = {3},
pages = {747-765},
year = {2015},
doi = {10.2466/23.25.PMS.120v17x2},
    note ={PMID: 26029968},

URL = { 
        https://doi.org/10.2466/23.25.PMS.120v17x2
    
},
eprint = { 
        https://doi.org/10.2466/23.25.PMS.120v17x2
    
}
,
    abstract = { Perturbations in Delayed Auditory Feedback (DAF) and speech rate were examined as sources of disruptions in speech between men and women. Fluent adult men (n = 16) and women (n = 16) spoke at a normal and an imposed fast rate of speech with 0, 25, 50, 100, and 200msec. DAF. The syllable rate significantly increased when participants were instructed to speak at a fast rate, and the syllable rate decreased with increasing DAF delays. Men's speech rate was significantly faster during the fast speech rate condition with a 200 msec. DAF. Disfluencies increased with increasing DAF delay. Significantly more disfluency occurred at delays of 25 and 50 msec. at the fast rate condition, while more disfluency occurred at 100 and 200msec. in normal rate conditions. Men and women did not display differences in the number of disfluencies. These findings demonstrate sex differences in susceptibility to perturbations in DAF and speech rate suggesting feedforward/feedback subsystems that monitor vocalizations may be different between sexes. }
}

speech delay paper 2
@article{doi:10.1121/1.1466868,
author = {Stuart,Andrew  and Kalinowski,Joseph  and Rastatter,Michael P.  and Lynch,Kerry },
title = {Effect of delayed auditory feedback on normal speakers at two speech rates},
journal = {The Journal of the Acoustical Society of America},
volume = {111},
number = {5},
pages = {2237-2241},
year = {2002},
doi = {10.1121/1.1466868},
URL = { 
        https://asa.scitation.org/doi/abs/10.1121/1.1466868
    
},
eprint = { 
        https://asa.scitation.org/doi/pdf/10.1121/1.1466868
    
}
}

fastspeech
@misc{fastspeech,
  doi = {10.48550/ARXIV.1905.09263},
  url = {https://arxiv.org/abs/1905.09263},
  author = {Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {FastSpeech: Fast, Robust and Controllable Text to Speech},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

emg2vec
@article{emg2vec,
  author    = {Huiyan Li and
               Haohong Lin and
               You Wang and
               Hengyang Wang and
               Han Gao and
               Qing Ai and
               Guang Li},
  title     = {Voice Reconstruction from Silent Speech with a Sequence-to-Sequence
               Model},
  journal   = {CoRR},
  volume    = {abs/2108.00190},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.00190},
  eprinttype = {arXiv},
  eprint    = {2108.00190},
  timestamp = {Thu, 24 Mar 2022 13:28:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-00190.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

n-gram def
@article{n_gram_def,
title = {Syntactic clustering of the Web},
journal = {Computer Networks and ISDN Systems},
volume = {29},
number = {8},
pages = {1157-1166},
year = {1997},
note = {Papers from the Sixth International World Wide Web Conference},
issn = {0169-7552},
doi = {https://doi.org/10.1016/S0169-7552(97)00031-7},
url = {https://www.sciencedirect.com/science/article/pii/S0169755297000317},
author = {Andrei Z. Broder and Steven C. Glassman and Mark S. Manasse and Geoffrey Zweig},
keywords = {Similarity, Duplication, Resemblance, Web search, Fingerprints, Signatures},
abstract = {We have developed an efficient way to determine the syntactic similarity of files and have applied it to every document on the World Wide Web. Using this mechanism, we built a clustering of all the documents that are syntactically similar. Possible applications include a “Lost and Found” service, filtering the results of Web searches, updating widely distributed web-pages, and identifying violations of intellectual property rights.}
}

Mel Scale - Formula
@book{mel_scale_formula,
  added-at = {2010-01-28T22:00:35.000+0100},
  author = {O'Shaughnessy, Douglas},
  biburl = {https://www.bibsonomy.org/bibtex/20f61e22fc189d0652d89de160d535b3b/zazi},
  description = {The Bibliography of my Belegarbeit},
  interhash = {27959f541a0c7ec6b4061ca150738466},
  intrahash = {0f61e22fc189d0652d89de160d535b3b},
  keywords = {imported},
  owner = {zazi},
  pages = 150,
  publisher = {{Addison-Wesley}},
  timestamp = {2010-01-28T22:00:37.000+0100},
  title = {{Speech communication: human and machine}},
  year = 1987
}

Silent Speech Interface (SSI) Definition
@article{ssi_definition,
title = {Silent speech interfaces},
journal = {Speech Communication},
volume = {52},
number = {4},
pages = {270-287},
year = {2010},
note = {Silent Speech Interfaces},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167639309001307},
author = {B. Denby and T. Schultz and K. Honda and T. Hueber and J.M. Gilbert and J.S. Brumberg},
keywords = {Silent speech, Speech pathologies, Cellular telephones, Speech recognition, Speech synthesis},
abstract = {The possibility of speech processing in the absence of an intelligible acoustic signal has given rise to the idea of a ‘silent speech’ interface, to be used as an aid for the speech-handicapped, or as part of a communications system operating in silence-required or high-background-noise environments. The article first outlines the emergence of the silent speech interface from the fields of speech production, automatic speech processing, speech pathology research, and telecommunications privacy issues, and then follows with a presentation of demonstrator systems based on seven different types of technologies. A concluding section underlining some of the common challenges faced by silent speech interface researchers, and ideas for possible future directions, is also provided.}
}

RNN Definition and Exploration
@article{rnn_fundamentals,
	doi = {10.1016/j.physd.2019.132306},
  
	url = {https://doi.org/10.1016%2Fj.physd.2019.132306},
  
	year = 2020,
	month = {mar},
  
	publisher = {Elsevier {BV}
},
  
	volume = {404},
  
	pages = {132306},
  
	author = {Alex Sherstinsky},
  
	title = {Fundamentals of Recurrent Neural Network ({RNN}) and Long Short-Term Memory ({LSTM}) network},
  
	journal = {Physica D: Nonlinear Phenomena}
}

CTC - Original
@inproceedings{ctc_original,
author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143891},
doi = {10.1145/1143844.1143891},
abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {369–376},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

Method - DropOut (DropConnect)
@InProceedings{pmlr-v28-wan13,
  title = 	 {Regularization of Neural Networks using DropConnect},
  author = 	 {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1058--1066},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wan13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wan13.html},
  abstract = 	 {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}


Dataset - LJSpeech
@misc{ljspeech17,
  author       = {Keith Ito and Linda Johnson},
  title        = {The LJ Speech Dataset},
  howpublished = {\url{https://keithito.com/LJ-Speech-Dataset/}},
  year         = 2017
}

Literature Review - Conformer Architecture
@misc{conformer,
  doi = {10.48550/ARXIV.2005.08100},
  url = {https://arxiv.org/abs/2005.08100},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  publisher = {arXiv},
  year = {2020},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


Literature Review - DeepSpeech2 Architecture
@misc{DS2_original,
  doi = {10.48550/ARXIV.1512.02595},
  url = {https://arxiv.org/abs/1512.02595},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Speech 2: End-to-End Speech Recognition in English and Mandarin},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


Literature Review - Original sEMG Systems
@article{lit_review_semg_history,
      author = {Wang, You and Zhang, Ming and Wu, Rumeng and Gao, Han and Yang, Meng and Luo, Zhiyuan and Li, Guang},
      year = {2020},
      month = {07},
      pages = {442},
      title = {Silent Speech Decoding Using Spectrogram Features Based on Neuromuscular Activities},
      volume = {10},
      journal = {Brain Sciences},
      doi = {10.3390/brainsci10070442}
}

Literature Review - Pruning
@misc{liebenwein2021lost,
      title={Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}, 
      author={Lucas Liebenwein and Cenk Baykal and Brandon Carter and David Gifford and Daniela Rus},
      year={2021},
      eprint={2103.03014},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

Literature Review - Quantization
@misc{wu2020integer,
      title={Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation}, 
      author={Hao Wu and Patrick Judd and Xiaojie Zhang and Mikhail Isaev and Paulius Micikevicius},
      year={2020},
      eprint={2004.09602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

OpenAI 2018 BlogPost - Details useful tools and other information which is used for the MLOps side of the project
@misc{OpenAI_dota,
  author = {OpenAI},
  title = {OpenAI Five},
  howpublished = {\url{https://web.archive.org/web/20210422020001/https://openai.com/blog/openai-five/}},
  year = {2018}
}

Efficient CapsNet - Might be very useful in the feature detection aspect of EEG dimensionality reduction and correlating features before
passing them on to a recurrent / transformer network
@misc{mazzia2021efficientcapsnet,
      title={Efficient-CapsNet: Capsule Network with Self-Attention Routing}, 
      author={Vittorio Mazzia and Francesco Salvetti and Marcello Chiaberge},
      year={2021},
      eprint={2101.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

Original CapsNet paper by Hinton, et al.
@misc{sabour2017dynamic,
      title={Dynamic Routing Between Capsules}, 
      author={Sara Sabour and Nicholas Frosst and Geoffrey E Hinton},
      year={2017},
      eprint={1710.09829},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

EfficientNet - Automatically adjusts baseline image classification network based on
required attributes of new model
@misc{tan2020efficientnet,
      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2020},
      eprint={1905.11946},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

ResNet - Original Paper, refer to this when describing digital voicing papers
and our new model which will definitely use some variation of this
@INPROCEEDINGS{resnet, 
 author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  
 booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
 title={Deep Residual Learning for Image Recognition},   
 year={2016},  
 volume={},  
 number={},  
 pages={770-778},  
 doi={10.1109/CVPR.2016.90}
}

Dota 2 Open AI Five Paper - Contains lots of generally useful information about large scale ML ideas
- Compute Usage formula taken from here
@misc{openai2019dota,
      title={Dota 2 with Large Scale Deep Reinforcement Learning}, 
      author={OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
      year={2019},
      eprint={1912.06680},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

ZFNet - Improves upon AlexNet and first visualisation of CNNs for image classification models
Also provides info for diagnosing performance of CNN image classification models
@misc{zeiler2013visualizing,
      title={Visualizing and Understanding Convolutional Networks}, 
      author={Matthew D Zeiler and Rob Fergus},
      year={2013},
      eprint={1311.2901},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

Levenshtein Distance (edit-distance between two words)
@ARTICLE{1966SPhD...10..707L,
       author = {{Levenshtein}, V.~I.},
        title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
      journal = {Soviet Physics Doklady},
         year = 1966,
        month = feb,
       volume = {10},
        pages = {707},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

SSI for Speech Restoration (Meta-Review of main SSIs, approaches, to-speech and to-text, biosignals)
Use this as a central resource to find references for specific SSI issues and to get a good overview of the silent speech literature in general
@article{ssi_meta_review,
  author = {Pérez-Córdoba, José and Martín-Doñas, Juan and Gomez, Angel and Gomez-Alanis, Alejandro and Gonzalez Lopez, Jose},
  year = {2020},
  month = {09},
  pages = {177995-178021},
  title = {Silent Speech Interfaces for Speech Restoration: A Review},
  volume = {8},
  journal = {IEEE Access},
  doi = {10.1109/ACCESS.2020.3026579}
}

Another potentially useful meta-review of biosignal sensors and DL-based speech recognition
@Article{s21041399,
  AUTHOR = {Lee, Wookey and Seong, Jessica Jiwon and Ozlu, Busra and Shim, Bong Sup and Marakhimov, Azizbek and Lee, Suan},
  TITLE = {Biosignal Sensors and Deep Learning-Based Speech Recognition: A Review},
  JOURNAL = {Sensors},
  VOLUME = {21},
  YEAR = {2021},
  NUMBER = {4},
  ARTICLE-NUMBER = {1399},
  URL = {https://www.mdpi.com/1424-8220/21/4/1399},
  ISSN = {1424-8220},
  ABSTRACT = {Voice is one of the essential mechanisms for communicating and expressing one’s intentions as a human being. There are several causes of voice inability, including disease, accident, vocal abuse, medical surgery, ageing, and environmental pollution, and the risk of voice loss continues to increase. Novel approaches should have been developed for speech recognition and production because that would seriously undermine the quality of life and sometimes leads to isolation from society. In this review, we survey mouth interface technologies which are mouth-mounted devices for speech recognition, production, and volitional control, and the corresponding research to develop artificial mouth technologies based on various sensors, including electromyography (EMG), electroencephalography (EEG), electropalatography (EPG), electromagnetic articulography (EMA), permanent magnet articulography (PMA), gyros, images and 3-axial magnetic sensors, especially with deep learning techniques. We especially research various deep learning technologies related to voice recognition, including visual speech recognition, silent speech interface, and analyze its flow, and systematize them into a taxonomy. Finally, we discuss methods to solve the communication problems of people with disabilities in speaking and future research with respect to deep learning components.},
  DOI = {10.3390/s21041399}
}

Digital Voicing of Silent Speech
@misc{gaddy2020digital,
      title={Digital Voicing of Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2020},
      eprint={2010.02960},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

Improves upon the original paper by:
- Replacing LSTMs with transformer network
- Generating time-aligned phoneme info from audio and using it as training signal
- Replace hand-designed features with convolution features
@misc{gaddy2021improved,
      title={An Improved Model for Voicing Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2021},
      eprint={2106.01933},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

Main MIT AlterEgo 2018 Project
@inproceedings{10.1145/3172944.3172977,
  author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
  title = {AlterEgo: A Personalized Wearable Silent Speech Interface},
  year = {2018},
  isbn = {9781450349451},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3172944.3172977},
  doi = {10.1145/3172944.3172977},
  abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92% median word accuracy levels.},
  booktitle = {23rd International Conference on Intelligent User Interfaces},
  pages = {43–53},
  numpages = {11},
  keywords = {intelligence augmentation, silent speech interface, human-machine symbiosis, peripheral nerve interface},
  location = {Tokyo, Japan},
  series = {IUI '18}
}

60 Page In-Depth Discussion of MIT AlterEgo 2018 Project
@inproceedings{Kapur2018HumanmachineCC,
  title={Human-machine cognitive coalescence through an internal duplex interface},
  author={Arnav Kapur},
  year={2018}
}

MIT AlterEgo Dysphonia Research (Silent speech in ambulatory setting with Multiple Sclerosis patients)
@InProceedings{pmlr-v116-kapur20a,
  title = {{Non-Invasive Silent Speech Recognition in Multiple Sclerosis with Dysphonia}},
  author = {Kapur, Arnav and Sarawgi, Utkarsh and Wadkins, Eric and Wu, Matthew and Hollenstein, Nora and Maes, Pattie},
  booktitle = {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = {25--38},
  year = {2020},
  editor = {Adrian V. Dalca and Matthew B.A. McDermott and Emily Alsentzer and Samuel G. Finlayson and Michael Oberst and Fabian Falck and Brett Beaulieu-Jones},
  volume = {116},
  series = {Proceedings of Machine Learning Research},
  address = {}, 
  month = {13 Dec}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v116/kapur20a/kapur20a.pdf}, 
  url = {http://proceedings.mlr.press/v116/kapur20a.html}, 
  abstract = {We present the first non-invasive real-time silent speech system that helps patients with speech disorders to communicate in natural language voicelessly, merely by articulating words or sentences in the mouth without producing any sounds. We collected neuromus-cular recordings to build a dataset of 10 trials of 15 sentences from each of 3 multiple sclerosis (MS) patients with dysphonia, spanning a range of severity and subsequently affected speech attributes. We present a pipeline wherein we carefully preprocess the data, develop a convolutional neural architecture and employ personalized machine learning. In our experiments with multiple sclerosis patients, our system achieves a mean overall test accuracy of 0.81 at a mean information transfer rate of 203.73 bits per minute averaged over all patients. Our work demonstrates the potential of a reliable and promising human-computer interface that classifies intended sentences from silent speech and hence, paves the path for future work with further speech disorders in conditions such as amyotrophic lateral sclerosis (ALS), stroke, and oral cancer, among others.}
}

MIT AlterEgo Continuous ASR via CTC
@phdthesis{alter_ego_ctc,
  title={A continuous silent speech recognition system for AlterEgo, a silent speech interface},
  author={Wadkins, Eric J},
  year={2019},
  school={Massachusetts Institute of Technology}
}

Speech Synthesis LSTM
@unknown{speech_synth_lstm,
author = {Bird, Jordan and Faria, Diego and Ekárt, A. and Premebida, Cristiano and Ayrosa, Pedro},
year = {2020},
month = {07},
pages = {},
title = {LSTM and GPT-2 Synthetic Speech Transfer Learning for Speaker Recognition to Overcome Data Scarcity}
}

GPT-2 EMG Synth
@ARTICLE{gpt_2_emg_synth, author={Bird, Jordan J. and Pritchard, Michael and Fratini, Antonio and Ekárt, Anikó and Faria, Diego R.},  journal={IEEE Robotics and Automation Letters},   title={Synthetic Biological Signals Machine-Generated by GPT-2 Improve the Classification of EEG and EMG Through Data Augmentation},   year={2021},  volume={6},  number={2},  pages={3498-3504},  doi={10.1109/LRA.2021.3056355}}

GPT-2
@article{gpt_2_original,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}