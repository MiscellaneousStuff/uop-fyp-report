Digital Voicing of Silent Speech
@misc{gaddy2020digital,
      title={Digital Voicing of Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2020},
      eprint={2010.02960},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

An Improved Model for Voicing Silent Speech
@misc{gaddy2021improved,
      title={An Improved Model for Voicing Silent Speech}, 
      author={David Gaddy and Dan Klein},
      year={2021},
      eprint={2106.01933},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

Main MIT AlterEgo 2018 Project
@inproceedings{10.1145/3172944.3172977,
  author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
  title = {AlterEgo: A Personalized Wearable Silent Speech Interface},
  year = {2018},
  isbn = {9781450349451},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3172944.3172977},
  doi = {10.1145/3172944.3172977},
  abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92% median word accuracy levels.},
  booktitle = {23rd International Conference on Intelligent User Interfaces},
  pages = {43â€“53},
  numpages = {11},
  keywords = {intelligence augmentation, silent speech interface, human-machine symbiosis, peripheral nerve interface},
  location = {Tokyo, Japan},
  series = {IUI '18}
}

60 Page In-Depth Discussion of MIT AlterEgo 2018 Project
@inproceedings{Kapur2018HumanmachineCC,
  title={Human-machine cognitive coalescence through an internal duplex interface},
  author={Arnav Kapur},
  year={2018}
}

MIT AlterEgo Dysphonia Research (Silent speech in ambulatory setting with Multiple Sclerosis patients)
@InProceedings{pmlr-v116-kapur20a,
  title = {{Non-Invasive Silent Speech Recognition in Multiple Sclerosis with Dysphonia}},
  author = {Kapur, Arnav and Sarawgi, Utkarsh and Wadkins, Eric and Wu, Matthew and Hollenstein, Nora and Maes, Pattie},
  booktitle = {Proceedings of the Machine Learning for Health NeurIPS Workshop},
  pages = {25--38},
  year = {2020},
  editor = {Adrian V. Dalca and Matthew B.A. McDermott and Emily Alsentzer and Samuel G. Finlayson and Michael Oberst and Fabian Falck and Brett Beaulieu-Jones},
  volume = {116},
  series = {Proceedings of Machine Learning Research},
  address = {}, 
  month = {13 Dec}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v116/kapur20a/kapur20a.pdf}, 
  url = {http://proceedings.mlr.press/v116/kapur20a.html}, 
  abstract = {We present the first non-invasive real-time silent speech system that helps patients with speech disorders to communicate in natural language voicelessly, merely by articulating words or sentences in the mouth without producing any sounds. We collected neuromus-cular recordings to build a dataset of 10 trials of 15 sentences from each of 3 multiple sclerosis (MS) patients with dysphonia, spanning a range of severity and subsequently affected speech attributes. We present a pipeline wherein we carefully preprocess the data, develop a convolutional neural architecture and employ personalized machine learning. In our experiments with multiple sclerosis patients, our system achieves a mean overall test accuracy of 0.81 at a mean information transfer rate of 203.73 bits per minute averaged over all patients. Our work demonstrates the potential of a reliable and promising human-computer interface that classifies intended sentences from silent speech and hence, paves the path for future work with further speech disorders in conditions such as amyotrophic lateral sclerosis (ALS), stroke, and oral cancer, among others.}
}