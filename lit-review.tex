\chapter{Literature Review} \label{chap:lit-review}

The purpose of this literature review is to familiarise readers with
basic concepts, key to understanding silent speech systems. Also the background
research for each of the approaches is briefly explored. However, each approach
also contains a further critical review of the literature for the chosen
method.

\section{Key Terms}

A list of silent speech and speech recognition domain specific key terms are provided
to familiarise the reader with certain concepts to help the reader digest this report.

\begin{itemize}
\item \emph{Word Error Rate (WER):}
Metric used in speech recognition systems which accounts for differences in length of
a recognised word sequence and a reference word sequence. WER is derived from
Levenshtein distance  which is a string metric for measuring the difference between two
sequences. The Levenshtein distance between two words is essentially the minimum number of
single-character edits (including insertions, deletions or substitutions) required to
change one word into another. Another term for it is the "edit distance".
\[
  WER
  = \dfrac{S + D + I}{N}
  = \dfrac{S + D + I}{S + D + C}
\]
Where each term means:\\
S: Number of substitutions\\
D: Number of deletions\\
I: Number of insertions\\
C: Number of correct words\\
N: Number of words in the reference (N = S+D+C)
(\cite{1966SPhD...10..707L})
\item \emph{Mel-Spectrogram:}
A regular spectrogram is a visualisation of the frequency spectrum
of a signal. Whereas a mel spectrogram is the same thing but the frequency
values use the mel scale rather than their raw values. The mel scale
is a scale of pitches judged by humans to be equal in distance from another
and is commonly used in speech processing systems as the scale normalises
the raw frequency values based on the scale of pitches. This makes it easier
to produce speech synthesis or speech recognition systems for machine
learning models.
\item \emph{Transduction/Transducer:}
A transducer is an electronic device that converts energy from one form
to another. In the context of this project, this refers to converting
signals recorded using electrodes from a persons body into speech signals.
\end{itemize}

\section{Silent Speech Transduction}

There are two primary landmark papers released in the silent speech domain
which this project takes large inspiration from. These are
"Digital Voicing of Silent Speech" (\cite{gaddy2020digital}) and
"An Improved Model for Voicing Silent Speech" (\cite{gaddy2021improved}).

This direction of research is the first successful attempt at transducing
speech directly from EMG signals recorded when a person is silently
articulating speech.

The original paper discusses how training silent speech models purely on biosignals
recorded during the silent articulation of speech leads to a poor training
signal due to differences between silent and vocalized articulation of
speech when transducing speech from this data. The main reason for 
this is the silent speech data has no time-aligned audio to use as a 
training target.

The original paper addresses this issue by using a set of utterances recorded during
silent and vocalized speech and finding alignments between the two recordings
and finding an alignment between both instances, using Dynamic
Time Warping.

The authors also released their dataset used in their research
to enable other authors to research silent speech. This makes
it very suitable for this project as not only is it free
to access, it has also been proven to be useful as the authors
are able to use it to create the first 
state-of-the-art transduction model for EMG silent speech.

Model learns its own input features directly from EMG signals
instead of hand-crafted features in prior work.
New model uses convolutional layers to extract features from the signals.
Transformer-encoder layers used to propagate signals across longer distances
instead of bi-directional LSTM layers.
Additional signal introduced during learning by introducing auxillary task of
predicting phoneme labels in addition to predicting speech audio features.
On open vocabulary intelligibility evaluation, the new model improves the absolute WER by 25.8\%.

The new state-of-the-art performance is established using
3 different methods compared to the previous paper.

\begin{itemize}
  \item \emph{Replace LSTMs with Transformer-Encoder:}
  Model bi-directional triple stacked 1024-unit LSTM layer replaced with 6 layer Transformer-Encoder
  layer to predict EMG features found using a ResNet inspired block
  over 1 second to predict speech along with phoneme loss signal.
  Ablation shows WER go from 42.2\% to 45.2\% when this is removed.
  \item \emph{Introduce Phoneme Loss:}
  Introduced auxillary phoneme prediction task as an additional 
  output of the self-attention transformer-encoder. This self-supervisory 
  signal is used to improve the performance of the network by learning to 
  predict the phoneme of the speech which is being detected and
  also regularises training.
  Ablation shows WER go from 42.2\% to 51.7\% when this is removed.
  \item \emph{Replace hand-designed features with convolution features:}
  Uses a CNN block based on ResNet to extract features from EMG signals which are end-to-end
  trainable instead of using hand-crafted features to improve the representational power of the network.
  Ablation shows WER go from 42.2\% to 46\% when this is removed.
\end{itemize}
(\cite{gaddy2021improved}).

\section{Data Augmentation via EMG Synthesis}

\subsection{Data Augmentation Background}

Data augmentation refers to techniques which are used to either increase
the amount of available training data by adding slightly modified
copies of the original data or to synthesize new examples entirely.
The purpose of this is to regularise the training and help reduce overfitting
when training a machine learning model (\cite{data_augmentation_def}).

Many methods exist to generate augmented data for machine learning.
One of them is to apply geometric transformations such as: translations, rotations,
cropping, flipping, scaling, etc. (\cite{data_augmentation_def}).

This method is commonly applied in the computer vision domain and
leads to substantial improvements in performance. For instance,
when cropping was applied to an image recognition dataset for the
Caltech 101 dataset, it lead to a Top-1 score increase of 13.82\%
(\cite{geometric_augment}).

\subsection{Applicability to EMG Domain}

However unlike images, EMG data is a collection of non-stationary
time-series data which has been collected from different electrodes
and also contains many other artefacts such as baseline noise,
powerline interference and other contamination (\cite{semg_filtering}).
Moreover, simple geometric transformations are not necessarily appropriate
for EMG as they may impact time domain features as EMG signals are
1-dimensional.

Further to this, in computer vision projects, it is easy to determine whether
an augmented dataset is similar to the original signal. However for EMG
signals, it is difficult to determine if an augmented dataset has still
retained key properties of the original dataset.
Figure \ref{fig:real-vs-pred-emg} provides a sample of an original
EMG data recording during vocalised speech and the synthesized EMG
data for an utterance from this project, to demonstrate this difficulty.
Although both of the data samples appear similar, when they are used in
a transduction model to produce speech, they sound noticably different from
each other because of the difference within the smaller time windows.

\begin{figure}[hbtp]
    \caption{Original vs Synthesized Vocal EMG Data from EMG Synthesis Approach}
    \label{fig:real-vs-pred-emg}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/emg_augment/real_vs_synth.png}
\end{figure}

Despite these issues with EMG synthesis, this work attempts different
methods to address these issues.

\section{Silent Speech Classification}

In recent years, there has been increasing research into systems
which can classify silent speech signals into text. One major approach
is the MIT AlterEgo (\cite{alter_ego}) which details a system which 
records biosignals recorded by electrodes placed on a person's face and
throat while a user is silently articulating speech (EMG).

Then these EMG signals are preprocessed, filtered and then processed
using a convolutional neural network (CNN) model (\cite{cnn_def}).

Although CNN models are typically used
to process 2-dimensional data, they are also very well suited
to processing 1-dimensional data and can be very efficient. One
paper found that using CNNs instead of deconvolution algorithms
to process digital signals results in a 65x speedup (\cite{cnn_dsp}).

The final accuracy of the MIT AlterEgo method 
was 92\% mean word accuracy across multiple
users. However, the vocabulary set represented used in this dataset
was very limited. This motivates the need to explore
EMG silent speech recognition for larger datasets, especially datasets
with larger vocabulary sizes.

\section{Summary}

From this literature review we can see that there are many promising approaches
for improving silent speech systems. This literature review only broadly
covers the silent speech literature and related topics.
The approaches section contain more detailed reviews which
specifically deal with the chosen approach and it's related literature.

Unless specified otherwise, silent speech dataset in this project
refers to the open-source Digital Voicing (\cite{gaddy2020digital})
dataset with the phoneme information
released with the second Digital Voincing paper (\cite{gaddy2021improved}).