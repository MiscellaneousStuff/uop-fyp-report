\iffalse
Mark Scheme:
- Extensive Research
- Understanding of complex subject matter
- Identifies flaws, gaps or inconsistencies in extant knowledge
\fi

\iffalse

Mu-Law compression:
- compresses audio signal into discrete bins whilst preserving dynamic range

Current sEMG Silent Speech Text Classification Research:
- https://dspace.mit.edu/bitstream/handle/1721.1/123121/1128187233-MIT.pdf?sequence=1&isAllowed=y

\fi

\chapter{Literature Review} \label{chap:lit-review}

\section{Key Terms - Silent Speech}

\begin{itemize}
  \item \emph{Word Error Rate (WER):}
  Metric used in speech recognition systems which accounts for differences in length of
  a recognised word sequence and a reference word sequence. WER is derived from
  Levenshtein distance  which is a string metric for measuring the difference between two
  sequences. The Levenshtein distance between two words is essentially the minimum number of
  single-character edits (including insertions, deletions or substitutions) required to
  change one word into another. Another term for it is the "edit distance".
  \[
    WER
    = \dfrac{S + D + I}{N}
    = \dfrac{S + D + I}{S + D + C}
  \]
  Where each term means:
  S: Number of substitutions\\
  D: Number of deletions\\
  I: Number of insertions\\
  C: Number of correct words\\
  N: Number of words in the reference (N = S+D+C)
  (\cite{1966SPhD...10..707L})
  \item \emph{Mel-Scale:}
  Perceptual scale of pitches judged by listeners to be equal in distance from one another.
  \[ m = 2595 \log_{10} \left(1 + \dfrac{f}{700}\right) \]
  \item \emph{Silent Speech Interface (SSI):}
  Fill in SSI definition here
  \item \emph{Mel-Frequency Cepstrum Co-efficient (MFCCs):}
  Representation of the short-term power spectrum of a sound, based on a linear cosine transform
  \item \emph{Surface Electromyography (sEMG):}
  Non-invasive, computer-based technique that records the electrical impulses placed on the surface
  of the skin overlying the nerve at rest (i.e. static) and during activity (i.e. dynamic).
\end{itemize}

\section{Silent Speech Interfaces for Speech Restoration: A Review}

\section{Word-Error Rate}



\section{Proposed Directions for Research}

\subsection{Text Classification}

\subsection{Data Augmentation}

It might be possible to use GANs (Generative Adversarial Networks) or
VAEs (Variational Auto Encoders) to create a model which can generate
more data samples.

One possible data augmentation method for sEMG based silent speech is to
reverse a SOTA transduction model. Existing transduction models for
EMG to speech use either EMG features or raw EMG data and transduce
it into speech features (typically mel spectrograms or MFCCs). However,
it may be possible to predict the EMG data from the mel spectrograms.

\subsection{Improving Existing Model}

Use RL-Learning to find a model which can outperform the original model
(problem is small dataset size which means that the benefits of using RL
to create a model might not be as applicable to this problem).

Use CapsNet on top of the CNNs to improve the generalisability of the
detected EEG features (there are papers which show that these are
feasible).

\section{Connectionist Temporal Classification (CTC)}

For any speech recognition task, a model must know the alignment
between the input (e.g. audio, EMG features, etc.) and the target
transcription. On the surface, this makes training any speech
recognition model difficult.

Without having the alignments between the input and the transcription,
simpler approaches aren't available to us, such as mapping a single
character to a fixed number of inputs. This is because people's rates
of speech vary, regardless of the input modality (e.g. audio, EMG features,
etc). Another option is to hand-label the alignments between the input
and the text transcriptions. This would produce a performant model, however,
hand-labeling the alignments is time consuming, especially for larger datasets.

Connectionist Temporal Classification (CTC) is a method to train a model
without knowing the alignment between the input and the output and is especially
well suited to labeled datasets such as speech recognition.