Throughout the course of this research, many different machine learning models were
trained to evaluate their performance on tasks. During these experiments,
the hyperparameters of the individual models were tweaked to improve the performance of
the models on the individual tasks. This section lists the final hyperparameters of
the individual networks.

\section{DeepSpeech2 Audio Hyperparameters}

% THESE NEED TO BE SET TO THE FINAL TRAINING RUNS VALUES

{\small\begin{center}
\captionof{table}{DeepSpeech2 Audio Model Hyperparameters}
\begin{tabular} { | l | c | c | c | }
\hline
Hyperparameter & Description & Value \\
\hline
n\_cnn\_layers & Number of CNN block layers & 3 \\
n\_rnn\_layers & Number of RNN block layers & 5 \\
n\_class & Number of model classes & Vocabulary size \\
n\_feats & Number of CNN block features & 128 \\
stride & CNN stride value & 2 \\
dropout & Dropout value & 0.1 \\
learning\_rate & Initial learning rate & 5e-4 \\
batch\_size & Examples per single minibatch & 20 \\
epochs & Number of full dataset training iterations & 500 \\
\hline
\end{tabular}
\end{center}}

\section{Silent Speech Transduction Hyperparameters}

\subsection{Initial Fine Tuning Experiments}

The hyperparameters for the transduction model were reduced from the original
SOTA model due to hardware constraints. In the Additional Reproducability
Information of the Improved Voicing (\cite{gaddy2021improved}) paper, they state
that to train the full model takes roughly 12 hours on an Nvidia Quadro RTX 6000
which has 24GB VRAM. The GPU I was experimenting on only had 8GB VRAM which meant
that I had to halve the number of transformer layers and reduce the hidden dimension
size.

{\small\begin{center}
\captionof{table}{SOTA Silent Speech Transduction Model}
\begin{tabular} { | l | c | c | c | }
\hline
Hyperparameter & Description & Value \\
\hline
n\_transformer\_layers & Number of Transformer Encoder layers & 3 \\
n\_class & Number of model classes & Vocabulary size \\
stride & ResNet Block stride value & 1 \\
learning\_rate & Initial learning rate & 1e-3 \\
batch\_size & Examples per single minibatch & 32 \\
epochs & Number of full dataset training iterations & 80 \\
model\_size & Size of hidden dimension for transformer encoder & 512 \\
\hline
\end{tabular}
\end{center}}