\section{Data Augmentation via EMG Synthesis}

One approach for improving the performance of any machine learning model
is to synthesize more data. This is particularly useful if the original
dataset is small and there are methods to synthesize more data.

\subsection{Related Work}

Previous approaches have used various deep learning techniques to synthesize
more EMG data to train EMG deep learning models. One approach
(\cite{gpt_2_emg_synth}) uses a GPT-2 (\cite{gpt_2_original}) like model
to synthesis EMG signals for simple action recognition such as grasp and
release (actions common to robotic prosthetics and manipulators). The inclusion
of synthesized EMG data during the training process improved the overall
gesture recognition accuracy from 68.29\% to 89.5\%.

Another related paper from the same author experiments with LSTM and GPT-2
models for synthesizing more speech for a speaker recogntion task. The
best model found by the authors for this task was a
3-layer, 128 hidden dimension LSTM network (\cite{speech_synth_lstm}).

\subsection{Research Design}

My formal hypothesis for this section is that it is possible to train
an EMG augmentation model for voiced EMG data which can improve the
performance of a regular transduction model by training on the
ground truth voiced EMG data and the synthesized voiced EMG data.

My research design involved experimenting with different hyperparameters
for my proposed network to see which values produced the lowest
loss value on the evaluation dataset. This metric was chosen as it
was the most objective measure of how the model was able to generalise
it's ability to produce novel EMG signals given a mel spectrogram input.

The loss function chosen for this task is mean squared error. This was
chosen for two reasons: firstly the original transduction model
from the Digital Voicing (\cite{gaddy2020digital}) paper uses this
loss function to transduce mel-frequency cepstrum co-efficients (MFCCs).
MFCCs are another common method of representing speech features.

Another
reason mean squared error was used was that recent research into
silent speech transduction uses an auxilliary loss function where the
transducer is given an additional task to predict the features of
the vocalised EMG input signal representation while it is presented
with a silent EMG input signal. The loss functions which the authors
chose was mean absolute error, however, they likely chose absolute
error instead of squared error because it has less of a drastic
effect on their overall loss calculation. For this task, we only
care about predicting the vocalised EMG signals given speech features
so mean squared error should provide a stronger signal to the overall
network (\cite{silent_speech_tonal}).

\subsection{Summary}

In summary, my hypothesis was disproved.
My initial hypothesis was that it was possible to simply reverse the
transduction network, which was introduced in the Digital Voicing paper
for transcribing from EMG features into speech features, but instead
reverse the features.
From my findings I found that this wasn't true because the low level 
features of the EMG data are difficult for the LSTM network to correctly
reproduce.

In hindsight my approach could have been improved by being more selective in
the early stages. I could have determined which electrode contributed the most
in the transduction task, and then tried to just synthesise the signals for that 
particular channel and then tried to synthesise an increasing number of
electrode channels.